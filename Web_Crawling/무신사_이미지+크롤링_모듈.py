# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ar_SNwEXhTSZhNmxUi52cpE_Yd7HeztK
"""

import requests
import shutil
from bs4 import BeautifulSoup
import pandas as pd

class MusinsaScraper:
    def __init__(self, query):
        self.query = query
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
        }
        self.url = "https://www.musinsa.com/search/musinsa/integration?q=" + self.query

    def scrape(self):
        response = requests.get(self.url, headers=self.headers)
        soup = BeautifulSoup(response.text, 'lxml')

        result = []
        image_urls = []

        goods_links = soup.find_all('a', attrs={'name': 'goods_link'})
        prices = soup.find_all('p', attrs={'class': 'price'})
        images = soup.find_all('img', attrs={'class': 'lazyload lazy'})

        for link, price, image in zip(goods_links, prices, images):
            title = link.get('title')
            price_text = price.find('del')

            if price_text is not None:
                price_text.extract()

            price_text = price.text.strip().replace('Ïõê', '').replace(',', '')
            link = link.get('href')
            image_url = image.get('data-original')

            if image_url.startswith('//'):
                image_url = 'https:' + image_url

            result.append((title, price_text, link))
            image_urls.append(image_url)

        df = pd.DataFrame(result, columns=['Product_Name', 'Price', 'Product_Link'])
        df['Price'] = df['Price'].str.replace(',', '').astype(int)

        for index, image_url in enumerate(image_urls):
            filename = f"image_{index}.jpg"
            self.download_image(image_url, filename)

        return df

    def download_image(self, url, filename):
        response = requests.get(url, stream=True)
        with open(filename, 'wb') as out_file:
            shutil.copyfileobj(response.raw, out_file)
        del response