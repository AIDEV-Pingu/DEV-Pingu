# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ar_SNwEXhTSZhNmxUi52cpE_Yd7HeztK
"""

import urllib.request
import json
import datetime
import pandas as pd

class NaverShoppingCrawler:
    def __init__(self, client_id, client_secret, keyWord):
        self.client_id = client_id
        self.client_secret = client_secret
        self.keyWord = keyWord

    def gen_search_url(self, api_node, start_num, disp_num):
        base = 'https://openapi.naver.com/v1/search'
        node = '/' + api_node + '.json'
        param_query = '?query=' + urllib.parse.quote(self.keyWord)
        param_start = '&start=' + str(start_num)
        param_disp = '&display=' + str(disp_num)
        return base + node + param_query + param_disp + param_start

    def get_result_onpage(self, url):
        request = urllib.request.Request(url)
        request.add_header('X-Naver-Client-Id', self.client_id)
        request.add_header('X-Naver-Client-Secret', self.client_secret)
        response = urllib.request.urlopen(request)
        print(f'{datetime.datetime.now()} Url Request Success')
        return json.loads(response.read().decode('utf-8'))

    def delete_tag(self, input_str):
        input_str = input_str.replace('<b>', '')
        input_str = input_str.replace('</b>', '')
        input_str = input_str.replace('\xa0', '')
        return input_str

    def get_fields(self, json_data):
        title = [self.delete_tag(each['title']) for each in json_data['items']]
        link = [each['link'] for each in json_data['items']]
        lprice = [each['lprice'] for each in json_data['items']]
        mall_name = [each['mallName'] for each in json_data['items']]
        result = pd.DataFrame({
            'title': title,
            'link': link,
            'lprice': lprice,
            'mall': mall_name,
        }, columns=['title','lprice','mall','link'])
        return result

    def run(self):
        result_datas = []
        for n in range(1, 1000, 100):
            url = self.gen_search_url('shop', n, 100)
            json_result = self.get_result_onpage(url)
            result = self.get_fields(json_result)
            result_datas.append(result)
        result_datas_concat = pd.concat(result_datas)
        result_datas_concat.reset_index(drop=True, inplace=True)
        #result_datas_concat['lprice'] = result_datas_concat['lprice'].str.replace(' ', '').astype(int)
        result_datas_concat.to_csv('/content/Naver_shopping.csv', sep=',', encoding="utf-8")